





import numpy as np
from data import train_data, test_data

vocab = set()
for text in train_data.keys():
    for w in text.split(" "):
        vocab.add(w)

vocab_size = len(vocab)
print(f'{vocab_size} unique words found')


# Assign indices to each word so RNN can work with them
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for i, w in enumerate(vocab)}

print(word_to_idx['good'])
print(idx_to_word[0])


#each input to the RNN is a vector -> need to represent our vocab as one-hot vectors 
#contain all zeros except for a single one. The “one” is at the word’s corresponding integer index.

def createInputs(text):
  '''
  Returns an array of one-hot vectors representing the words
  in the input text string.
  - text is a string
  - Each one-hot vector has shape (vocab_size, 1)
  '''
    inputs = []
    for w in text.split(" "):
        v = np.zeros(vocab_size, 1)
        v[word_to_idx[w]] = 1
        inputs.append(v)
    return inputs





from numpy.random import randn

class RNN:
    
    def __init__(self, input_size, output_size, hidden_size=64):
        #initialize weights
        self.Whh = randn(hidden_size, hidden_size) / 1000 #weight matrix for hidden-to-hidden connections
        self.Wxh = randn(hidden_size, input_size) / 1000 #weight matrix for input-to-hidden connections
        self.Why = randn(output_size, hidden_size) / 1000 #weight matrix for hidden-to-output connections

        #initialize biases (usually set to zero)
        self.bh = np.zeros(hidden_size, 1)
        self.by = np.zereos(output_size, 1)

    def forward(self, inputs):
        '''
        Perform a forward pass of the RNN using the given inputs.
        Returns the final output and hidden state.
            - inputs is an array of one-hot vectors with shape (input_size, 1).
        '''
        h = np.zeros((self.Whh.shape[0], 1)) #hidden state vector (same dim as weight matrix). Initialized to zeros.
        
        #multiply inputs by weights (use @ for matrix mulitiplication), add bias, apply activation function (tanh)
        for i, x in enumerate(inputs):
            # x:  A single one-hot input vector (shape (input_size, 1)).
            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)
            #self.Wxh @ x: contribution of the current input to the new hidden state.
            #self.Whh @ h: contribution of the previous hidden state to the new hidden state.
        #calculate output
        y = self.Why @ h + self.by
        return y, h

def softmax(xs):
    # Applies the Softmax Function to the input array.
    return np.exp(xs) / sum(np.exp(xs))



